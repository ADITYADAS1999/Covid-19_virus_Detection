{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled20.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## Import Necessary Libraries ##"
      ],
      "metadata": {
        "id": "XtiWSUNnmQKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import ResNet50"
      ],
      "metadata": {
        "id": "oY_xPUj9Kl0C"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import keras\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import  Flatten, Dense, Dropout\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.applications import resnet\n",
        "model = resnet.ResNet50\n",
        "from keras.models import Model, Input\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization, AveragePooling2D, GlobalAveragePooling2D\n",
        "import keras_preprocessing\n",
        "from keras_preprocessing import image\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "from keras import callbacks\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "EiT-1q_0JG2k"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x\n",
        "print(tf.__version__)\n",
        "print(tf.test.gpu_device_name())\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoaIXlkdJ_ZE",
        "outputId": "cb67f067-da3b-4329-a807-89d8bf3d6ec9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n",
            "\n",
            "Num GPUs Available:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Fetch Dataset ##"
      ],
      "metadata": {
        "id": "yURTGtkbmaHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/drive/MyDrive/covid_data\"\n",
        "!kaggle datasets download -d plameneduardo/sarscov2-ctscan-dataset\n",
        "!unzip \\*.zip  && rm *.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOtxtViqJG5K",
        "outputId": "3c141c43-1db6-4e05-834e-791ad36b712b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/kaggle/__init__.py\", line 23, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/kaggle/api/kaggle_api_extended.py\", line 166, in authenticate\n",
            "    self.config_file, self.config_dir))\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /content/drive/MyDrive/covid_data. Or use the environment method.\n",
            "unzip:  cannot find or open *.zip, *.zip.zip or *.zip.ZIP.\n",
            "\n",
            "No zipfiles found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Split Into Train Data & Test Data ##"
      ],
      "metadata": {
        "id": "dyPqgGzQmitS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disease_types=['COVID', 'non-COVID']\n",
        "data_dir = '/content/drive/MyDrive/covid_data'\n",
        "train_dir = os.path.join(data_dir)"
      ],
      "metadata": {
        "id": "oz0h_1KFJG7q"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = []\n",
        "for defects_id, sp in enumerate(disease_types):\n",
        "    for file in os.listdir(os.path.join(train_dir, sp)):\n",
        "        train_data.append(['{}/{}'.format(sp, file), defects_id, sp])      \n",
        "train = pd.DataFrame(train_data, columns=['File', 'DiseaseID','Disease Type'])"
      ],
      "metadata": {
        "id": "8A2DlKZMJG-M"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = 64\n",
        "def read_image(filepath):\n",
        "    return cv2.imread(os.path.join(data_dir, filepath)) \n",
        "def resize_image(image, image_size):\n",
        "    return cv2.resize(image.copy(), image_size, interpolation=cv2.INTER_AREA)"
      ],
      "metadata": {
        "id": "tD5LtD5lJHAr"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.zeros((train.shape[0], IMAGE_SIZE, IMAGE_SIZE, 3))\n",
        "for i, file in tqdm(enumerate(train['File'].values)):\n",
        "    image = read_image(file)\n",
        "    if image is not None:\n",
        "        X_train[i] = resize_image(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "X_Train = X_train / 255.\n",
        "print(X_Train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQZeUJf_JHDN",
        "outputId": "42c5932a-2ea9-4555-a1ec-9c39a2a84e3d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2481it [05:35,  7.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2481, 64, 64, 3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train = train['DiseaseID'].values\n",
        "Y_train = to_categorical(Y_train, num_classes=2)"
      ],
      "metadata": {
        "id": "0pQ8Az8xMmEc"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, Y_train, Y_val = train_test_split(X_Train, Y_train, test_size=0.2, random_state = 42)"
      ],
      "metadata": {
        "id": "hkg-un73MmG0"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Define VGG16 Model Architecture ##"
      ],
      "metadata": {
        "id": "wBFwbdBOmoyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS = 100\n",
        "SIZE=64\n",
        "N_ch=3"
      ],
      "metadata": {
        "id": "ecN_FlzQMmJS"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_resnet50():\n",
        "    resnet50 = ResNet50(weights='imagenet', include_top=False)\n",
        "\n",
        "    input = Input(shape=(SIZE, SIZE, N_ch))\n",
        "    x = Conv2D(3, (3, 3), padding='same')(input)\n",
        "    \n",
        "    x = resnet50(x)\n",
        "    \n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    # multi output\n",
        "    output = Dense(2,activation = 'softmax', name='root')(x)\n",
        " \n",
        "\n",
        "    # model\n",
        "    model = Model(input,output)\n",
        "    \n",
        "    optimizer = Adam(lr=0.003, beta_1=0.9, beta_2=0.999, epsilon=0.1, decay=0.0)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "Ji7K9rG8MmLs"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Train the Model ##"
      ],
      "metadata": {
        "id": "m2hSE4nGmwVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_resnet50()\n",
        "annealer = ReduceLROnPlateau(monitor='val_accuracy', factor=0.70, patience=5, verbose=1, min_lr=1e-4)\n",
        "checkpoint = ModelCheckpoint('ResNet50_Model.hdf5', verbose=1, save_best_only=True)\n",
        "datagen = ImageDataGenerator(rotation_range=360, \n",
        "                        width_shift_range=0.2, \n",
        "                        height_shift_range=0.2,\n",
        "                        zoom_range=0.2, \n",
        "                        horizontal_flip=True, \n",
        "                        vertical_flip=True) \n",
        "\n",
        "datagen.fit(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78ZNl321MmOD",
        "outputId": "b71c78dd-7d7b-4470-c6f8-ab276d581490"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 0s 0us/step\n",
            "94781440/94765736 [==============================] - 0s 0us/step\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 64, 64, 3)]       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 64, 64, 3)         84        \n",
            "                                                                 \n",
            " resnet50 (Functional)       (None, None, None, 2048)  23587712  \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 2048)             0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 2048)             8192      \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 2048)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               524544    \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 256)              1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " root (Dense)                (None, 2)                 514       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24,122,070\n",
            "Trainable params: 24,064,342\n",
            "Non-trainable params: 57,728\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=BATCH_SIZE),\n",
        "               steps_per_epoch=X_train.shape[0] // BATCH_SIZE,\n",
        "               epochs=EPOCHS,\n",
        "               verbose=1,\n",
        "               callbacks=[annealer, checkpoint],\n",
        "               validation_data=(X_val, Y_val))"
      ],
      "metadata": {
        "id": "u3MjoSHXNCT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Plot the Results ##"
      ],
      "metadata": {
        "id": "eZbzSk2Bm3BK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']"
      ],
      "metadata": {
        "id": "leRdBEslNCWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(len(acc))\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('TraiN & Val Acc VS Epochs')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IePM5t6bNCYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(len(acc))\n",
        "plt.plot(epochs, loss, 'r', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "plt.title('TraiN & Val Loss VS Epochs')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lYKTxMA8OKAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(\"ResNet50_Model.hdf5\")\n",
        "score = model.evaluate(X_val, Y_val ,verbose=1)\n",
        "print('Test Loss:', score[0])\n",
        "print('Test accuracy:', score[1]*100)"
      ],
      "metadata": {
        "id": "ie3_S2ZxOKCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred = model.predict(X_val)\n",
        "Y_predx = np.argmax(Y_pred, axis = -1)\n",
        "Y_valx = np.argmax(Y_val, axis = -1)\n",
        "cf_matrix = confusion_matrix(Y_valx, Y_predx)\n",
        "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
        "                cf_matrix.flatten()]\n",
        "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
        "                     cf_matrix.flatten()/np.sum(cf_matrix)]\n",
        "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
        "          zip(group_names,group_counts,group_percentages)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "sns.heatmap(cf_matrix, annot = labels, fmt = '')\n",
        "plt.title(\"Confusion Matrix\")"
      ],
      "metadata": {
        "id": "5e2mb1bfOKIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wsUq-z4JOKLA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}