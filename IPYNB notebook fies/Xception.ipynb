{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Xception.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBsPCMvepoqN"
      },
      "outputs": [],
      "source": [
        "## Import Necessary Libraries ##"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import ResNet50"
      ],
      "metadata": {
        "id": "bXZYSfMrpqUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import keras\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import  Flatten, Dense, Dropout\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.applications import resnet\n",
        "model = resnet.ResNet50\n",
        "from keras.models import Model, Input\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization, AveragePooling2D, GlobalAveragePooling2D\n",
        "import keras_preprocessing\n",
        "from keras_preprocessing import image\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "from keras import callbacks\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "rZdRg0n3pqVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x\n",
        "print(tf.__version__)\n",
        "print(tf.test.gpu_device_name())\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "id": "X5gT5-WEpqWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Fetch Dataset ##"
      ],
      "metadata": {
        "id": "w41TXCBspqac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/MyDrive/COVID Chest X-Ray Analysis\"\n",
        "!kaggle datasets download -d plameneduardo/sarscov2-ctscan-dataset\n",
        "!unzip \\*.zip  && rm *.zip"
      ],
      "metadata": {
        "id": "2eI_9TF_pqdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Split Into Train Data & Test Data ##"
      ],
      "metadata": {
        "id": "sXflShClpqfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disease_types=['COVID', 'non-COVID']\n",
        "data_dir = '/content/gdrive/MyDrive/COVID Chest X-Ray Analysis/Dataset'\n",
        "train_dir = os.path.join(data_dir)"
      ],
      "metadata": {
        "id": "8El6eH8wpqm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = []\n",
        "for defects_id, sp in enumerate(disease_types):\n",
        "    for file in os.listdir(os.path.join(train_dir, sp)):\n",
        "        train_data.append(['{}/{}'.format(sp, file), defects_id, sp])      \n",
        "train = pd.DataFrame(train_data, columns=['File', 'DiseaseID','Disease Type'])"
      ],
      "metadata": {
        "id": "HYpBEn67pqpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = 64\n",
        "def read_image(filepath):\n",
        "    return cv2.imread(os.path.join(data_dir, filepath)) \n",
        "def resize_image(image, image_size):\n",
        "    return cv2.resize(image.copy(), image_size, interpolation=cv2.INTER_AREA)"
      ],
      "metadata": {
        "id": "vsNCSR6apqrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.zeros((train.shape[0], IMAGE_SIZE, IMAGE_SIZE, 3))\n",
        "for i, file in tqdm(enumerate(train['File'].values)):\n",
        "    image = read_image(file)\n",
        "    if image is not None:\n",
        "        X_train[i] = resize_image(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "X_Train = X_train / 255.\n",
        "print(X_Train.shape)"
      ],
      "metadata": {
        "id": "SRw23JqbpqtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train = train['DiseaseID'].values\n",
        "Y_train = to_categorical(Y_train, num_classes=2)"
      ],
      "metadata": {
        "id": "fjd93Iqrpqu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, Y_train, Y_val = train_test_split(X_Train, Y_train, test_size=0.2, random_state = 42)"
      ],
      "metadata": {
        "id": "um8INmM6pqwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Define Xception Model Architecture ##"
      ],
      "metadata": {
        "id": "1kvGDX8Epqyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS = 100\n",
        "SIZE=64\n",
        "N_ch=3\n"
      ],
      "metadata": {
        "id": "lmG5RSOUpq0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_Xception():\n",
        "    xception = Xception(weights='imagenet', include_top=False)\n",
        "\n",
        "    input = Input(shape=(SIZE, SIZE, N_ch))\n",
        "    x = Conv2D(3, (3, 3), padding='same')(input)\n",
        "    \n",
        "    x = xception(x)\n",
        "    \n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    # multi output\n",
        "    output = Dense(2,activation = 'softmax', name='root')(x)\n",
        " \n",
        "\n",
        "    # model\n",
        "    model = Model(input,output)\n",
        "    \n",
        "    optimizer = Adam(lr=0.003, beta_1=0.9, beta_2=0.999, epsilon=0.1, decay=0.0)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "d88pSQytpq3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_Xception()\n",
        "annealer = ReduceLROnPlateau(monitor='val_accuracy', factor=0.70, patience=5, verbose=1, min_lr=1e-4)\n",
        "checkpoint = ModelCheckpoint('Xception_Model.hdf5', verbose=1, save_best_only=True)\n",
        "datagen = ImageDataGenerator(rotation_range=360, \n",
        "                        width_shift_range=0.2, \n",
        "                        height_shift_range=0.2,\n",
        "                        zoom_range=0.2, \n",
        "                        horizontal_flip=True, \n",
        "                        vertical_flip=True) \n",
        "\n",
        "datagen.fit(X_train)"
      ],
      "metadata": {
        "id": "G9EIF3lfpq4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=BATCH_SIZE),\n",
        "               steps_per_epoch=X_train.shape[0] // BATCH_SIZE,\n",
        "               epochs=EPOCHS,\n",
        "               verbose=1,\n",
        "               callbacks=[annealer, checkpoint],\n",
        "               validation_data=(X_val, Y_val))"
      ],
      "metadata": {
        "id": "B_wYqgJppq6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']"
      ],
      "metadata": {
        "id": "4633cYy5qh2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(len(acc))\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('TraiN & Val Acc VS Epochs')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "moQOQl0lqh5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(len(acc))\n",
        "plt.plot(epochs, loss, 'r', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "plt.title('TraiN & Val Loss VS Epochs')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uqkzVOEiqh7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(\"Xception_Model.hdf5\")\n",
        "score = model.evaluate(X_val, Y_val ,verbose=1)\n",
        "print('Test Loss:', score[0])\n",
        "print('Test accuracy:', score[1]*100)"
      ],
      "metadata": {
        "id": "P62j60eRqh9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred = model.predict(X_val)\n",
        "Y_predx = np.argmax(Y_pred, axis = -1)\n",
        "Y_valx = np.argmax(Y_val, axis = -1)\n",
        "cf_matrix = confusion_matrix(Y_valx, Y_predx)\n",
        "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
        "                cf_matrix.flatten()]\n",
        "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
        "                     cf_matrix.flatten()/np.sum(cf_matrix)]\n",
        "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
        "          zip(group_names,group_counts,group_percentages)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "sns.heatmap(cf_matrix, annot = labels, fmt = '')\n",
        "plt.title(\"Confusion Matrix\")"
      ],
      "metadata": {
        "id": "toSJR35Yqh_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QvYc2mFDqiBi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}